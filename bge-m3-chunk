import os
import asyncio
import json
import random 
import re 
from typing import List, Dict, Any, Optional
from dataclasses import dataclass
from datetime import datetime, timezone
import sys
import httpx # ğŸš¨ NEW: Ø¨Ø±Ø§ÛŒ ÙØ±Ø§Ø®ÙˆØ§Ù†ÛŒ API Cloudflare
from docx import Document
from docx.table import Table
from docx.text.paragraph import Paragraph
from dotenv import load_dotenv, find_dotenv

# --- Ú©ØªØ§Ø¨Ø®Ø§Ù†Ù‡â€ŒÙ‡Ø§ÛŒ Ø¶Ø±ÙˆØ±ÛŒ ---
try:
    from supabase import create_client, Client
    # ğŸš¨ Cohere Client Ø¨Ø±Ø§ÛŒ Embedding Ø­Ø°Ù Ø´Ø¯.
    # import cohere 
except ImportError:
    print("ğŸ›‘ Critical: Ensure you have 'pip install supabase python-dotenv python-docx httpx'")
    sys.exit(1)

# ----------------------------------------
# Û±. ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ùˆ Ù…ØªØºÛŒØ±Ù‡Ø§ÛŒ Ù…Ø­ÛŒØ·ÛŒ
# ----------------------------------------
load_dotenv(find_dotenv(), override=True)

SUPABASE_URL = os.getenv("SUPABASE_URL")
SUPABASE_SERVICE_KEY = os.getenv("SUPABASE_SERVICE_KEY")

# ğŸš¨ğŸš¨ ØªÙ†Ø¸ÛŒÙ…Ø§Øª CLOUDFLARE BGE-M3
CLOUDFLARE_API_TOKEN = os.getenv("CLOUDFLARE_API_TOKEN")
CLOUDFLARE_ACCOUNT_ID = os.getenv("CLOUDFLARE_ACCOUNT_ID")
CLOUDFLARE_EMBED_MODEL = "@cf/baai/bge-m3"

# ğŸš¨ Ø§Ø¨Ø¹Ø§Ø¯ Ù†Ù‡Ø§ÛŒÛŒ ØªÙ†Ø¸ÛŒÙ… Ø´Ø¯Ù‡ Ø¯Ø± Supabase - Ø¨Ø§ÛŒØ¯ 1024 Ø¨Ø§Ø´Ø¯!
FALLBACK_EMBED_DIM = 1024 

# ğŸ’¡ ØªÙ†Ø¸ÛŒÙ…Ø§Øª Semantic Chunking
MAX_CHUNK_SIZE = 700 
CHUNK_OVERLAP = 150 

# ----------------------------------------
# Û². Ù†Ù…ÙˆÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ú©Ù„Ø§ÛŒÙ†Øªâ€ŒÙ‡Ø§ Ùˆ Ø¨Ø±Ø±Ø³ÛŒ ØªÙ†Ø¸ÛŒÙ…Ø§Øª
# ----------------------------------------
if not all([SUPABASE_URL, SUPABASE_SERVICE_KEY, CLOUDFLARE_API_TOKEN, CLOUDFLARE_ACCOUNT_ID]): 
    print("ğŸ›‘ Critical: Supabase settings and CLOUDFLARE keys must be set.")
    sys.exit(1)
    
try:
    supabase: Client = create_client(SUPABASE_URL, SUPABASE_SERVICE_KEY)
    print("âœ… Supabase client initialized.")
except Exception as e:
    print(f"âŒ Configuration Error: Could not initialize Supabase client: {e}")
    sys.exit(1)

# ----------------------------------------
# Û³. Ú©Ù„Ø§Ø³â€ŒÙ‡Ø§ÛŒ Ø³Ø±ÙˆÛŒØ³ LLM (Ø¨Ø§ Ú©Ù„Ø§ÛŒÙ†Øª CLOUDFLARE)
# ----------------------------------------

@dataclass
class ProcessedChunk:
    url: str
    chunk_number: int
    title: str
    summary: str
    content: str
    metadata: Dict[str, Any]
    embedding: Optional[List[float]] 

class CloudflareEmbedClient:
    """Ú©Ù„Ø§Ø³ Ø¨Ø±Ø§ÛŒ Ù…Ø¯ÛŒØ±ÛŒØª ØªÙ…Ø§Ø³â€ŒÙ‡Ø§ÛŒ Ù†Ø§Ù‡Ù…Ú¯Ø§Ù… Ùˆ Ù…Ø³ØªÙ‚ÛŒÙ… Ø¨Ø§ API Cloudflare BGE-M3."""
    
    def __init__(self, api_token: str, account_id: str, embed_model: str, dim: int):
        self.api_token = api_token
        self.account_id = account_id
        self.embed_model = embed_model
        self.dim = dim
        self.api_url = f"https://api.cloudflare.com/client/v4/accounts/{account_id}/ai/run/{embed_model}"
        self.headers = {
            "Authorization": f"Bearer {api_token}",
            "Content-Type": "application/json"
        }
        # ğŸš¨ Ø§ÛŒØ¬Ø§Ø¯ AsyncClient
        self.client = httpx.AsyncClient(timeout=30) 
        
    async def embed_content_direct(self, text: str) -> Optional[List[float]]:
        """Ø¯Ø±ÛŒØ§ÙØª Embedding Ø¨Ø§ Cloudflare API."""
        
        MAX_RETRIES = 3 
        base_delay = 5 
        
        # ğŸš¨ BGE-M3 Ø¨Ø±Ø§ÛŒ Ú†Ø§Ù†Ú©â€ŒÙ‡Ø§ Ø¨Ù‡ Ù¾ÛŒØ´ÙˆÙ†Ø¯ "passage: " Ù†ÛŒØ§Ø² Ø¯Ø§Ø±Ø¯
        payload = {"text": [f"passage: {text}"]}

        for attempt in range(MAX_RETRIES):
            try:
                response = await self.client.post(
                    self.api_url, 
                    headers=self.headers, 
                    json=payload
                )
                response.raise_for_status() # Ø®Ø·Ø§Ù‡Ø§ Ø±Ø§ Ø¨Ù‡ Exception ØªØ¨Ø¯ÛŒÙ„ Ù…ÛŒâ€ŒÚ©Ù†Ø¯
                
                data = response.json()
                vector = data.get("result", {}).get("data", [[]])[0]
                
                if not vector or len(vector) != self.dim:
                    print(f"âŒ Cloudflare returned {len(vector)} dims, expected {self.dim}. Skipping.")
                    return None
                
                return vector
                    
            except httpx.HTTPStatusError as e:
                # 429: Too Many Requests, 5xx: Server Errors
                if e.response.status_code in [429, 500, 503] and attempt < MAX_RETRIES - 1:
                    delay = base_delay * (2 ** attempt) + random.uniform(0, 1) 
                    print(f" Â  âš ï¸ Cloudflare API Server Error ({e.response.status_code}). Retrying in {delay:.2f}s (Attempt {attempt + 1}/{MAX_RETRIES}).")
                    await asyncio.sleep(delay)
                    continue 
                
                print(f"âŒ Cloudflare API Error {e.response.status_code}. Details: {e.response.text[:100]}...")
                return None
            
            except Exception as e:
                print(f"âŒ General Error during API call: {type(e).__name__}: {e}.")
                return None
        
        return None 
        
    async def close(self):
        """Ø¨Ø³ØªÙ† Ú©Ù„Ø§ÛŒÙ†Øª httpx."""
        await self.client.aclose()

# Ù†Ù…ÙˆÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ù†Ù‡Ø§ÛŒÛŒ Ú©Ù„Ø§ÛŒÙ†Øª
cloudflare_embed_client = CloudflareEmbedClient( 
    CLOUDFLARE_API_TOKEN, 
    CLOUDFLARE_ACCOUNT_ID,
    CLOUDFLARE_EMBED_MODEL,
    FALLBACK_EMBED_DIM
)

# ----------------------------------------
# Û´. ØªÙˆØ§Ø¨Ø¹ Ø§Ø¨Ø²Ø§Ø±ÛŒ Ùˆ Ù‡Ø³ØªÙ‡ RAG
# ----------------------------------------

def simple_text_heuristic(chunk: str) -> Dict[str, str]:
    # ... (Ø¨Ø¯ÙˆÙ† ØªØºÛŒÛŒØ±) ...
    lines = [l.strip() for l in chunk.splitlines() if l.strip()]
    
    if lines:
        title = lines[0].lstrip('#*->- ').strip()[:80]
        
        sentences = re.split(r'(?<=[.!?])\s+', " ".join(lines))
        summary = " ".join([s.strip() for s in sentences[:3] if s.strip()])[:300]
        
        if not summary and lines:
            summary = " ".join(lines[:2])[:300]

    else:
        title = "Untitled (Empty Chunk)"
        summary = "No content to summarize."
        
    return {"title": title, "summary": summary.strip()}


def chunk_text_by_sentence(text: str, max_size: int = MAX_CHUNK_SIZE, overlap: int = CHUNK_OVERLAP) -> List[str]:
    # ... (Ø¨Ø¯ÙˆÙ† ØªØºÛŒÛŒØ±) ...
    sentences = re.split(r'(?<=[.?!])\s+', text)
    if not sentences:
        return []

    chunks = []
    current_chunk = ""
    
    for sentence in sentences:
        sentence = sentence.strip()
        if not sentence:
            continue
            
        if len(current_chunk) + len(sentence) + 1 > max_size and current_chunk:
            chunks.append(current_chunk.strip())
            
            overlap_sentences = []
            overlap_len = 0
            
            temp_sentences = re.split(r'(?<=[.?!])\s+', current_chunk)
            
            for s in reversed(temp_sentences):
                s = s.strip()
                if not s: continue
                
                if overlap_len + len(s) + 1 <= overlap:
                    overlap_sentences.insert(0, s) 
                    overlap_len += len(s) + 1
                else:
                    break
            
            current_chunk = " ".join(overlap_sentences).strip()
            
            current_chunk = (current_chunk + " " + sentence).strip()
            
        else:
            current_chunk = (current_chunk + " " + sentence).strip()

    if current_chunk:
        chunks.append(current_chunk.strip())
        
    return chunks


async def insert_chunk(chunk: ProcessedChunk):
    
    if not supabase: return None
    
    if chunk.embedding is None or len(chunk.embedding) != FALLBACK_EMBED_DIM: 
        print(f"ğŸ›‘ Skipping insertion for chunk {chunk.chunk_number} (Invalid embedding length: {len(chunk.embedding) if chunk.embedding else 0}).")
        return None
        
    try:
        data = {
            "url": chunk.url,
            "chunk_number": chunk.chunk_number,
            "title": chunk.title,
            "summary": chunk.summary,
            "content": chunk.content,
            "metadata": chunk.metadata,
            "embedding": chunk.embedding
        }
        
        result = supabase.table("site_pages").insert(data).execute()
        
        print(f"âœ… Inserted chunk {chunk.chunk_number} for {chunk.url}")
        return result
    except Exception as e:
        if 'duplicate key value violates unique constraint' in str(e):
             print(f"âš ï¸ Insertion Skipped: Chunk {chunk.chunk_number} for {chunk.url} already exists in DB.")
        else:
             print(f"âŒ Error inserting chunk: {e}") 
        return None

async def process_chunk(chunk: str, chunk_number: int, url: str) -> Optional[ProcessedChunk]:
    
    fallback_data = simple_text_heuristic(chunk)
    title = fallback_data["title"]
    summary = fallback_data["summary"]
    llm_summary_used = False # LLM Summary is not used here (just simple heuristic)

    # ğŸš¨ BGE-M3 Ø§Ø² ØªØ±Ú©ÛŒØ¨ Ø¹Ù†ÙˆØ§Ù†/Ø®Ù„Ø§ØµÙ‡ Ùˆ Ù…Ø­ØªÙˆØ§ Ø¨Ø±Ø§ÛŒ Embedding Ù¾Ø´ØªÛŒØ¨Ø§Ù†ÛŒ Ù…ÛŒâ€ŒÚ©Ù†Ø¯
    meta_chunk = f"Title: {title}\nSummary: {summary}\n\nCONTENT:\n{chunk}"

    # ğŸš¨ ÙØ±Ø§Ø®ÙˆØ§Ù†ÛŒ Ú©Ù„Ø§ÛŒÙ†Øª Cloudflare
    embedding = await cloudflare_embed_client.embed_content_direct(meta_chunk)
    
    if embedding is None or len(embedding) != FALLBACK_EMBED_DIM:
        print(f"ğŸ›‘ CRITICAL: Embedding failed or had wrong dimension for chunk {chunk_number}. Skipping insertion.")
        return None 
        
    return ProcessedChunk(
        url=url,
        chunk_number=chunk_number,
        title=title,
        summary=summary,
        content=chunk,
        embedding=embedding,
        metadata={
            "source": url,
            "chunk_length": len(chunk),
            "llm_summary_used": llm_summary_used,
            "embedding_model": CLOUDFLARE_EMBED_MODEL, 
            "retrieved_at": datetime.now(timezone.utc).isoformat()
        }
    )


def extract_table_text(table: Table) -> str:
    """Ø¬Ø¯ÙˆÙ„ Ø±Ø§ Ø¨Ù‡ ÙØ±Ù…Øª Ù…ØªÙ†ÛŒ Ù‚Ø§Ø¨Ù„ Ø®ÙˆØ§Ù†Ø¯Ù† ØªØ¨Ø¯ÛŒÙ„ Ù…ÛŒâ€ŒÚ©Ù†Ø¯."""
    table_data = []
    
    # ğŸš¨ Ø§ÛŒØ¬Ø§Ø¯ Ù‡Ø¯Ø± Ø¨Ø±Ø§ÛŒ Ø®ÙˆØ§Ù†Ø§ÛŒÛŒ Ø¨Ù‡ØªØ± Ø¬Ø¯ÙˆÙ„ (Ù…Ø«Ù„Ø§Ù‹: Table X: ...)
    table_data.append("--- TABLE START ---")
    
    for i, row in enumerate(table.rows):
        row_text = []
        for j, cell in enumerate(row.cells):
            # ØªÙ…ÛŒØ² Ú©Ø±Ø¯Ù† Ù…ØªÙ† Ù‡Ø± Ø³Ù„ÙˆÙ„
            cell_content = cell.text.replace('\n', ' ').strip()
            # ÙØ±Ù…Øª: [Column 1: Value] [Column 2: Value]
            row_text.append(f"[C{j+1}: {cell_content}]")
            
        table_data.append(f"Row {i+1}: {' '.join(row_text)}")
        
    table_data.append("--- TABLE END ---")
    return '\n'.join(table_data)

def extract_text_from_docx(file_path: str) -> str:
    """Extracts all text content (including tables) from a .docx file."""
    try:
        document = Document(file_path)
        full_content = []
        
        for element in document.element.body:
            # ğŸš¨ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù¾Ø§Ø±Ø§Ú¯Ø±Ø§Ù
            if element.tag.endswith('p'): # Paragraph
                paragraph = Paragraph(element, document)
                text = paragraph.text.strip()
                if text:
                    full_content.append(text)
            
            # ğŸš¨ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø¬Ø¯ÙˆÙ„
            elif element.tag.endswith('tbl'): # Table
                table = Table(element, document)
                table_text = extract_table_text(table)
                if table_text:
                    full_content.append('\n' + table_text + '\n')
                    
        return '\n\n'.join(full_content)
    except Exception as e:
        print(f"âŒ Error extracting text from DOCX: {e}")
        return ""


async def process_and_store_document(url: str, content: str):
    
    chunks = chunk_text_by_sentence(content) 
    print(f"Divided document into {len(chunks)} semantic chunks (Max Size: {MAX_CHUNK_SIZE}, Overlap: {CHUNK_OVERLAP}).")
    
    processed_chunks = []
    
    for i, chunk in enumerate(chunks):
        
        if not chunk.strip():
            print(f"âš ï¸ Skipping empty chunk {i}.")
            continue
            
        print(f"\nâš™ï¸ Processing Chunk {i} of {len(chunks)} (Length: {len(chunk)})...")
        
        try:
            processed_chunk = await process_chunk(chunk, i, url)
            
            if processed_chunk is not None:
                processed_chunks.append(processed_chunk)
                
            if processed_chunk is not None and i < len(chunks) - 1:
                # ğŸš¨ Ø¯Ø± Ø§ÛŒÙ†Ø¬Ø§ Ù†ÛŒØ§Ø²ÛŒ Ø¨Ù‡ ØªØ§Ø®ÛŒØ± Ø²ÛŒØ§Ø¯ Ù†Ø¯Ø§Ø±ÛŒÙ…ØŒ Ù…Ú¯Ø± Ø§ÛŒÙ†Ú©Ù‡ Cloudflare Rate Limit Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´Ø¯.
                # ØªØ§Ø®ÛŒØ± Ø±Ø§ Ú©Ù… Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…:
                delay = 0.5 + random.uniform(0, 0.5) 
                print(f"â³ Waiting {delay:.2f} seconds after successful embedding...")
                await asyncio.sleep(delay)
                
        except Exception as e:
            print(f"âŒ Critical error during chunk processing {i}: {type(e).__name__}: {e}. Skipping chunk.")
            
    
    print("\nStarting SERIAL insertion to Supabase...")
    for chunk in processed_chunks:
        await insert_chunk(chunk)
    

async def process_local_file(file_path: str):
    
    content = ""
    source_name = os.path.basename(file_path)
    
    # ğŸš¨ Ø¨Ù‡Ø¨ÙˆØ¯ ÛŒØ§ÙØªÙ‡ Ø¨Ø±Ø§ÛŒ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¬Ø¯ÙˆÙ„ Ø§Ø² docx
    if file_path.lower().endswith('.docx'):
        print(f"ğŸ“„ Processing DOCX: {source_name}")
        content = extract_text_from_docx(file_path)
    elif file_path.lower().endswith(('.txt', '.md')):
          print(f"ğŸ“„ Processing Text/MD: {source_name}")
          try:
              with open(file_path, "r", encoding="utf-8") as f:
                  content = f.read()
          except Exception as e:
              print(f"âŒ Error reading file: {e}")

    content_length = len(content.strip())
    print(f"ğŸ’¡ Extracted content length: {content_length} characters.")
    
    if not content or content_length == 0:
        print(f"ğŸ›‘ Process stopped: Extracted content from {source_name} is empty or extraction failed.")
        return

    source_url = f"local://{source_name}"
    await process_and_store_document(source_url, content)
    
    print(f"\nâœ… Finished RAG pipeline for local file: {file_path}")

# ----------------------------------------
# Ûµ. ØªØ§Ø¨Ø¹ Ø§ØµÙ„ÛŒ
# ----------------------------------------
async def main():
    
    # ğŸš¨ Ù…Ø³ÛŒØ± ÙØ§ÛŒÙ„ Ø±Ø§ Ø¨Ø±Ø±Ø³ÛŒ Ú©Ù†ÛŒØ¯.
    local_document_file = r"YOURFILE"
    
    if not os.path.exists(local_document_file):
        print(f"âŒ Document file not found at: {local_document_file}")
        return
    
    print(f"ğŸš€ Starting RAG Pipeline | Embed Model: {CLOUDFLARE_EMBED_MODEL} ({FALLBACK_EMBED_DIM} Dim)")
    print("-----------------------------------------------------------------")
    
    if not CLOUDFLARE_API_TOKEN or not CLOUDFLARE_ACCOUNT_ID:
        print("\nğŸ›‘ FATAL ERROR: CLOUDFLARE API keys are missing. Check your .env file.")
        return

    try:
        await process_local_file(local_document_file)
        
    except Exception as e:
        print(f"\nâŒ Pipeline failed during execution. **Detailed Error:** {type(e).__name__}: {e}")
        
    finally:
        print("\nğŸ§¹ Closing clients...")
        # ğŸš¨ ÙØ±Ø§Ø®ÙˆØ§Ù†ÛŒ Ù…ØªØ¯ close Ø§ØµÙ„Ø§Ø­ Ø´Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ httpx
        await cloudflare_embed_client.close()


if __name__ == "__main__":
    
    try:
        # ğŸš¨ Ù…Ø·Ù…Ø¦Ù† Ø´ÙˆÛŒØ¯ Ú©Ù‡ asyncio.run ÙÙ‚Ø· ÛŒÚ© Ø¨Ø§Ø± ÙØ±Ø§Ø®ÙˆØ§Ù†ÛŒ Ù…ÛŒâ€ŒØ´ÙˆØ¯
        asyncio.run(main())
    except ValueError as e:
        print(f"âŒ Initialization Error: {e}")
