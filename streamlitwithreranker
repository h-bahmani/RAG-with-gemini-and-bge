import os
import sys
import httpx 
import re 
from typing import List, Dict, Any, Optional
from dotenv import load_dotenv, find_dotenv
import streamlit as st
from supabase.client import create_client, Client
from datetime import datetime

# ---------------- Streamlit Configuration (MUST BE FIRST COMMAND) ----------------
st.set_page_config(page_title="Custom Gemini Rerank Chatbot", layout="wide")

# ---------------- ENV & CONFIG ----------------
load_dotenv(find_dotenv(), override=True)


CLOUDFLARE_API_TOKEN = os.getenv("CLOUDFLARE_API_TOKEN") 
CLOUDFLARE_ACCOUNT_ID = os.getenv("CLOUDFLARE_ACCOUNT_ID") 

EMBED_MODEL = "@cf/baai/bge-m3" 
EMBED_DIM = 1024 

RERANK_MODEL = "Custom Gemini Reranker" 
GEMINI_MODEL = "gemini-2.5-flash" 
GOOGLE_API_KEY = os.environ.get("GOOGLE_API_KEY")

SUPABASE_URL = os.getenv("SUPABASE_URL")
SUPABASE_SERVICE_KEY = os.getenv("SUPABASE_SERVICE_KEY")

# ğŸš¨ ØªÙ†Ø¸ÛŒÙ…Ø§Øª Custom Reranking
TOP_K_RETRIEVAL = 10  # â¬…ï¸ Ø¨Ø§Ø²ÛŒØ§Ø¨ÛŒ 10 Ú†Ø§Ù†Ú© Ø§ÙˆÙ„ÛŒÙ‡ Ø¨Ø±Ø§ÛŒ Ø±ØªØ¨Ù‡â€ŒØ¨Ù†Ø¯ÛŒ Ø¨Ù‡ØªØ± ØªÙˆØ³Ø· Gemini
TOP_K_RERANK = 5      # Ú†Ø§Ù†Ú©â€ŒÙ‡Ø§ÛŒ Ù†Ù‡Ø§ÛŒÛŒ Ú©Ù‡ Ø¨Ù‡ LLM Ù…ÛŒâ€ŒØ±Ø³Ù†Ø¯

if not all([GOOGLE_API_KEY, SUPABASE_URL, SUPABASE_SERVICE_KEY, CLOUDFLARE_API_TOKEN, CLOUDFLARE_ACCOUNT_ID]):
    st.error("âŒ ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ù†Ø§Ù‚Øµ: Ú©Ù„ÛŒØ¯Ù‡Ø§ÛŒ API (Google/Cloudflare) Ùˆ Supabase Ø±Ø§ Ø¯Ø± ÙØ§ÛŒÙ„ .env ØªÙ†Ø¸ÛŒÙ… Ú©Ù†ÛŒØ¯.")
    
# Initialize Supabase 
supabase = None
if SUPABASE_URL and SUPABASE_SERVICE_KEY:
    try:
        supabase: Client = create_client(SUPABASE_URL, SUPABASE_SERVICE_KEY)
    except Exception as e:
        st.error(f"âŒ Ø®Ø·Ø§ÛŒ Ø§ØªØµØ§Ù„ Ø¨Ù‡ Supabase: {e}")

# ---------------- CLOUDFLARE EMBEDDING CLIENT ----------------

class CloudflareBaseClient:
    def __init__(self, api_token: str, account_id: str):
        self.api_url_base = f"https://api.cloudflare.com/client/v4/accounts/{account_id}/ai/run"
        self.headers = {
            "Authorization": f"Bearer {api_token}",
            "Content-Type": "application/json"
        }
        self.client = httpx.Client(timeout=180) 

class CloudflareEmbedClient(CloudflareBaseClient):
    """Ú©Ù„Ø§ÛŒÙ†Øª Embedding Ø¨Ø±Ø§ÛŒ Cloudflare BGE-M3 (1024 Dim)."""
    
    def __init__(self, api_token: str, account_id: str):
        super().__init__(api_token, account_id)
        self.model = EMBED_MODEL
        self.dim = EMBED_DIM 
        self.api_url = f"{self.api_url_base}/{self.model}"

    def embed(self, text: str) -> List[float]:
        payload = {"text": [f"query: {text}"]}
        
        try:
            response = self.client.post(self.api_url, headers=self.headers, json=payload)
            response.raise_for_status() 
            
            data = response.json()
            vector = data.get("result", {}).get("data", [[]])[0]
            
            if not vector or len(vector) != self.dim:
                return [0.0] * self.dim
            
            return vector
                
        except Exception as e:
            st.error(f"âŒ General Embed API Error: {e}")
            return [0.0] * self.dim


# ---------------- LLM Client (Gemini Generation) ----------------

class GeminiClient:
    GEMINI_API_BASE_URL = "https://generativelanguage.googleapis.com"
    
    def __init__(self, api_key: str):
        self.api_key = api_key
        self.model = GEMINI_MODEL 
        self.api_path = f"{self.GEMINI_API_BASE_URL}/v1beta/models/{self.model}:generateContent"
        self.full_url = self.api_path 
        self.headers = {"Content-Type": "application/json"}
        self.client = httpx.Client(timeout=180)

    def generate(self, system_prompt: str, history: List[Dict[str, str]], user_prompt: str) -> str:
        
        if not self.api_key:
             return "Ø®Ø·Ø§: Ú©Ù„ÛŒØ¯ GOOGLE_API_KEY ØªÙ†Ø¸ÛŒÙ… Ù†Ø´Ø¯Ù‡ Ø§Ø³Øª."
             
        # ... (Ù…Ù†Ø·Ù‚ ØªØ¨Ø¯ÛŒÙ„ ØªØ§Ø±ÛŒØ®Ú†Ù‡ Ø¨Ù‡ ÙØ±Ù…Øª Gemini) ...
        api_history = []
        for msg in history:
            role = "user" if msg["role"] == "user" else "model"
            if msg.get("content"):
                api_history.append({"role": role, "parts": [{"text": msg["content"]}]})

        contents = [
            {"role": "user", "parts": [{"text": system_prompt}]},
            {"role": "model", "parts": [{"text": "Ø¨Ø§Ø´Ù‡ØŒ Ø¯Ø±Ú© Ø´Ø¯."}]}
        ]
        
        # Ø§Ú¯Ø± ØªØ§Ø±ÛŒØ®Ú†Ù‡ Ù…Ú©Ø§Ù„Ù…Ù‡ ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´Ø¯ØŒ Ø¢Ù† Ø±Ø§ Ø§Ø¶Ø§ÙÙ‡ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…
        if api_history:
             # ØªØ§Ø±ÛŒØ®Ú†Ù‡ Ù…Ú©Ø§Ù„Ù…Ù‡ ÙˆØ§Ù‚Ø¹ÛŒ Ø±Ø§ Ø§Ø¶Ø§ÙÙ‡ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ… (Ø§Ø² Ù¾ÛŒØ§Ù… Ø¯ÙˆÙ… Ø¨Ù‡ Ø¨Ø¹Ø¯ Ø§Ú¯Ø± Ø§ÙˆÙ„ÛŒÙ† Ù¾ÛŒØ§Ù… Ø¯Ø³ØªÛŒØ§Ø± Ø§Ø³Øª)
            contents.extend(api_history[1:]) 
        
        contents.append({"role": "user", "parts": [{"text": user_prompt}]})
        
        payload = {"contents": contents}
        params = {"key": self.api_key} 

        try:
            r = self.client.post(self.full_url, headers=self.headers, json=payload, params=params)
            
            if not r.is_success:
                msg = r.json().get("error", {}).get("message", r.text)
                st.error(f"Gemini API Error {r.status_code}: {msg}")
                return "Ø®Ø·Ø§ Ø¯Ø± Ø¨Ø±Ù‚Ø±Ø§Ø±ÛŒ Ø§Ø±ØªØ¨Ø§Ø· Ø¨Ø§ Ù…Ø¯Ù„ Gemini. (Ù„Ø·ÙØ§Ù‹ GOOGLE_API_KEY Ø±Ø§ Ú†Ú© Ú©Ù†ÛŒØ¯)"
                
            data = r.json()
            # ğŸš¨ Ø¨Ø±Ø±Ø³ÛŒ Ø®Ø·Ø§Ù‡Ø§ÛŒ Ø§Ø­ØªÙ…Ø§Ù„ÛŒ Ø§Ø² Ø·Ø±Ù LLM (Ù…Ø«Ù„Ø§Ù‹ Blocked by safety settings)
            if 'candidates' not in data or not data['candidates']:
                return "Ø®Ø·Ø§: Gemini Ù¾Ø§Ø³Ø®ÛŒ Ø¨Ø±Ù†Ú¯Ø±Ø¯Ø§Ù†Ø¯ ÛŒØ§ Ù…Ø­ØªÙˆØ§ ØªÙˆØ³Ø· ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø§ÛŒÙ…Ù†ÛŒ Ø¨Ù„Ø§Ú© Ø´Ø¯Ù‡ Ø§Ø³Øª."
                
            return data.get("candidates", [{}])[0].get("content", {}).get("parts", [{}])[0].get("text", "") or "No response."
        except Exception as e:
            st.error(f"LLM Connection Error: {e}")
            return "Ø®Ø·Ø§ÛŒ Ø´Ø¨Ú©Ù‡ ÛŒØ§ Ø§ØªØµØ§Ù„ Ø¨Ù‡ Gemini."


# ---------------- CLIENT INITIALIZATION ----------------

@st.cache_resource
def get_clients():
    """Ø§ÛŒØ¬Ø§Ø¯ ÛŒÚ©Ø¨Ø§Ø±Û€ Ú©Ù„Ø§ÛŒÙ†Øªâ€ŒÙ‡Ø§ÛŒ Cloudflare Embed Ùˆ Gemini."""
    if not CLOUDFLARE_API_TOKEN or not CLOUDFLARE_ACCOUNT_ID:
        st.error("âŒ Cloudflare API keys are missing. Embedding is disabled.")
        embed_client = None
    else:
        embed_client = CloudflareEmbedClient(CLOUDFLARE_API_TOKEN, CLOUDFLARE_ACCOUNT_ID)
        
    gemini_client = GeminiClient(GOOGLE_API_KEY)
    
    st.success("âœ… Clients Initialized (Cloudflare Embed & Gemini LLM/Reranker).", icon="ğŸ§ ")
    return embed_client, gemini_client

# ğŸš¨ ÙØ±Ø§Ø®ÙˆØ§Ù†ÛŒ Ú©Ù„Ø§ÛŒÙ†Øªâ€ŒÙ‡Ø§ Ø¨Ø±Ø§ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø¯Ø± Ø³Ø·Ø­ Ù…Ø§Ú˜ÙˆÙ„
cloudflare_embed_service, gemini_service = get_clients()


# ---------------- CUSTOM RERANKING FUNCTION (Gemini-based) ----------------

def custom_rerank_with_gemini(query: str, documents: List[Dict[str, Any]], top_k: int) -> List[Dict[str, Any]]:
    """
    Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Gemini 2.5 Flash Ø¨Ø±Ø§ÛŒ Ø§Ù†ØªØ®Ø§Ø¨ top_k Ú†Ø§Ù†Ú© Ù…Ø±ØªØ¨Ø·
    Ø§Ø² Ù„ÛŒØ³Øª retrieved_chunks (Ø¨Ø§ ÛŒÚ© ÙØ±Ø§Ø®ÙˆØ§Ù†ÛŒ LLM).
    """
    if gemini_service is None:
        st.error("âŒ Gemini service not initialized for custom reranking.")
        return documents[:top_k]

    # 1. Ø³Ø§Ø®Øª Ù„ÛŒØ³Øª Ú†Ø§Ù†Ú©â€ŒÙ‡Ø§ Ø¨Ø±Ø§ÛŒ LLM
    numbered_chunks = []
    for i, doc in enumerate(documents):
        # Ø´Ù…Ø§Ø±Ù‡â€ŒÚ¯Ø°Ø§Ø±ÛŒ Ú†Ø§Ù†Ú©â€ŒÙ‡Ø§ Ø¨Ø±Ø§ÛŒ Ø¨Ø§Ø²Ú¯Ø´Øª Ø¨Ù‡ Ø§ÛŒÙ†Ø¯Ú©Ø³ Ø§ØµÙ„ÛŒ
        content = doc.get('content', '').replace('\n', ' ').strip()
        # ğŸš¨ ÙÙ‚Ø· 700 Ú©Ø§Ø±Ø§Ú©ØªØ± Ø§ÙˆÙ„ Ø±Ø§ Ø¨Ø±Ø§ÛŒ Ù¾Ø±Ø§Ù…Ù¾Øª Ù…ÛŒâ€ŒÙØ±Ø³ØªÛŒÙ… ØªØ§ ØªÙˆÚ©Ù† Ú©Ù…ØªØ±ÛŒ Ù…ØµØ±Ù Ø´ÙˆØ¯
        numbered_chunks.append(f"[Chunk {i+1}]: {content[:700]}...") 

    chunks_list = "\n---\n".join(numbered_chunks)

    # 2. Ù¾Ø±Ø§Ù…Ù¾Øª Ø¯Ù‡ÛŒ Ø¨Ù‡ Gemini Ø¨Ø±Ø§ÛŒ Ø§Ù†ØªØ®Ø§Ø¨ Ùˆ Ù…Ø±ØªØ¨â€ŒØ³Ø§Ø²ÛŒ
    rerank_prompt = (
        "Ø´Ù…Ø§ ÛŒÚ© Ø±ØªØ¨Ù‡â€ŒØ¨Ù†Ø¯ÛŒâ€ŒÚ©Ù†Ù†Ø¯Ù‡ (Reranker) Ù…ØªØ®ØµØµ Ù‡Ø³ØªÛŒØ¯. ÙˆØ¸ÛŒÙÙ‡ Ø´Ù…Ø§ Ø§ÛŒÙ† Ø§Ø³Øª Ú©Ù‡ Ø§Ø² Ù„ÛŒØ³Øª Ú†Ø§Ù†Ú©â€ŒÙ‡Ø§ÛŒ "
        f"Ø§Ø±Ø§Ø¦Ù‡ Ø´Ø¯Ù‡ Ø¯Ø± Ø²ÛŒØ±ØŒ **{top_k}** Ù…ÙˆØ±Ø¯ Ø±Ø§ Ú©Ù‡ **Ø¨ÛŒØ´ØªØ±ÛŒÙ† Ø§Ø±ØªØ¨Ø§Ø· Ù…Ø¹Ù†Ø§ÛŒÛŒ Ùˆ Ø¨Ø§Ù„Ø§ØªØ±ÛŒÙ† Ú©ÛŒÙÛŒØª Ø§Ø·Ù„Ø§Ø¹Ø§Øª** Ø±Ø§ Ø¨Ø§ Ø³Ø¤Ø§Ù„ Ú©Ø§Ø±Ø¨Ø± Ø¯Ø§Ø±Ù†Ø¯ØŒ "
        "Ø§Ù†ØªØ®Ø§Ø¨ Ú©Ù†ÛŒØ¯. Ù‡Ù†Ú¯Ø§Ù… Ø±ØªØ¨Ù‡â€ŒØ¨Ù†Ø¯ÛŒØŒ Ø§ÛŒÙ† Ù…Ø¹ÛŒØ§Ø±Ù‡Ø§ Ø±Ø§ Ø¨Ù‡ ØªØ±ØªÛŒØ¨ Ø§ÙˆÙ„ÙˆÛŒØª Ø±Ø¹Ø§ÛŒØª Ú©Ù†ÛŒØ¯:\n"
        "1. **Ø§Ø±ØªØ¨Ø§Ø· Ù…Ø³ØªÙ‚ÛŒÙ…:** Ú†Ø§Ù†Ú©ÛŒ Ø±Ø§ Ø§Ù†ØªØ®Ø§Ø¨ Ú©Ù†ÛŒØ¯ Ú©Ù‡ Ø´Ø§Ù…Ù„ Ù¾Ø§Ø³Ø® Ù…Ø³ØªÙ‚ÛŒÙ… Ø¨Ù‡ Ø³Ø¤Ø§Ù„ Ø¨Ø§Ø´Ø¯ØŒ Ù†Ù‡ ÙÙ‚Ø· Ú©Ù„Ù…Ø§Øª Ú©Ù„ÛŒØ¯ÛŒ Ù…Ø´ØªØ±Ú©.\n"
        "2. **Ø¬Ø§Ù…Ø¹ÛŒØª:** Ú†Ø§Ù†Ú©ÛŒ Ø±Ø§ Ø§Ù†ØªØ®Ø§Ø¨ Ú©Ù†ÛŒØ¯ Ú©Ù‡ Ú©Ø§Ù…Ù„â€ŒØªØ±ÛŒÙ† Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø±Ø§ Ø¯Ø± Ù…ÙˆØ±Ø¯ Ù…ÙˆØ¶ÙˆØ¹ Ø³Ø¤Ø§Ù„ Ø§Ø±Ø§Ø¦Ù‡ Ø¯Ù‡Ø¯.\n"
        "3. **ØªÚ©Ø±Ø§Ø±Ø²Ø¯Ø§ÛŒÛŒ:** Ø§Ø² Ø§Ù†ØªØ®Ø§Ø¨ Ú†Ø§Ù†Ú©â€ŒÙ‡Ø§ÛŒÛŒ Ú©Ù‡ Ù…Ø­ØªÙˆØ§ÛŒ Ù…Ø´Ø§Ø¨Ù‡ ÛŒØ§ ØªÚ©Ø±Ø§Ø±ÛŒ Ø¯Ø§Ø±Ù†Ø¯ Ø®ÙˆØ¯Ø¯Ø§Ø±ÛŒ Ú©Ù†ÛŒØ¯ Ùˆ Ú†Ø§Ù†Ú©â€ŒÙ‡Ø§ÛŒ Ù…Ù†Ø­ØµØ±Ø¨Ù‡â€ŒÙØ±Ø¯ Ø±Ø§ ØªØ±Ø¬ÛŒØ­ Ø¯Ù‡ÛŒØ¯.\n"
        "4. **Ú©ÛŒÙÛŒØª:** Ú†Ø§Ù†Ú©â€ŒÙ‡Ø§ÛŒÛŒ Ú©Ù‡ Ø¬Ù…Ù„Ø§Øª Ú©Ø§Ù…Ù„ Ùˆ Ø³Ø§Ø®ØªØ§Ø± ÙˆØ§Ø¶Ø­ÛŒ Ø¯Ø§Ø±Ù†Ø¯ Ø±Ø§ Ø¨Ø§Ù„Ø§ØªØ± Ø±ØªØ¨Ù‡â€ŒØ¨Ù†Ø¯ÛŒ Ú©Ù†ÛŒØ¯."
        "\n\n**Ù¾Ø§Ø³Ø® Ø´Ù…Ø§ Ø¨Ø§ÛŒØ¯ ÙÙ‚Ø· Ø´Ø§Ù…Ù„ Ø´Ù…Ø§Ø±Ù‡â€ŒÙ‡Ø§ÛŒ Ú†Ø§Ù†Ú©â€ŒÙ‡Ø§ÛŒ Ø§Ù†ØªØ®Ø§Ø¨ÛŒ (Ø¨Ù‡ ØªØ±ØªÛŒØ¨ Ø±ØªØ¨Ù‡ Ø¨Ø§Ù„Ø§ØªØ± Ø¨Ù‡ Ù¾Ø§ÛŒÛŒÙ†â€ŒØªØ±)ØŒ "
        "Ø¨Ø§ Ú©Ø§Ù…Ø§ (,) Ø¬Ø¯Ø§ Ø´Ø¯Ù‡ Ø¨Ø§Ø´Ø¯. Ù‡ÛŒÚ† Ù…ØªÙ† ÛŒØ§ ØªÙˆØ¶ÛŒØ­ÛŒ Ø§Ø¶Ø§ÙÙ‡ Ù†Ú©Ù†ÛŒØ¯.**"
        "\n\n**Ø³Ø¤Ø§Ù„ Ú©Ø§Ø±Ø¨Ø±:**\n"
        f"'{query}'"
        "\n\n**Ù„ÛŒØ³Øª Ú†Ø§Ù†Ú©â€ŒÙ‡Ø§ Ø¨Ø±Ø§ÛŒ Ø±ØªØ¨Ù‡â€ŒØ¨Ù†Ø¯ÛŒ:**\n"
        f"{chunks_list}"
        "\n\n**Ù¾Ø§Ø³Ø® (ÙÙ‚Ø· Ø´Ù…Ø§Ø±Ù‡ Ú†Ø§Ù†Ú©â€ŒÙ‡Ø§):**"
    )
    
    try:
        selection_text = gemini_service.generate(
            system_prompt="Ø´Ù…Ø§ ÛŒÚ© Ø§Ø¨Ø²Ø§Ø± Ø±ØªØ¨Ù‡â€ŒØ¨Ù†Ø¯ÛŒ Ù‡Ø³ØªÛŒØ¯ Ùˆ ÙÙ‚Ø· Ø´Ù…Ø§Ø±Ù‡â€ŒÙ‡Ø§ Ø±Ø§ Ø¨Ø±Ù…ÛŒâ€ŒÚ¯Ø±Ø¯Ø§Ù†ÛŒØ¯.",
            history=[], 
            user_prompt=rerank_prompt
        )
        
        # 3. Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø®Ø±ÙˆØ¬ÛŒ LLM
        # Ø®Ø±ÙˆØ¬ÛŒ ØªÙ…ÛŒØ² Ø´Ø¯Ù‡ (ÙÙ‚Ø· Ø§Ø¹Ø¯Ø§Ø¯ Ùˆ Ú©Ø§Ù…Ø§)
        selected_indices_str = re.findall(r'(\d+)', selection_text)
        
        if not selected_indices_str:
            st.warning(f"âš ï¸ Custom Reranker failed to extract numbers. LLM output: {selection_text[:100]}... Falling back to Similarity.")
            # Ø§Ú¯Ø± LLM Ù†ØªÙˆØ§Ù†Ø³Øª Ø§Ø¹Ø¯Ø§Ø¯ Ø±Ø§ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ú©Ù†Ø¯ØŒ Ø¨Ø± Ø§Ø³Ø§Ø³ Similarity Ø§ÙˆÙ„ÛŒÙ‡ Ø¨Ø±Ù…ÛŒâ€ŒÚ¯Ø±Ø¯Ø§Ù†ÛŒÙ…
            documents.sort(key=lambda x: x.get('similarity', 0.0), reverse=True)
            return documents[:top_k]
            
        selected_indices = []
        
        # 4. Ø¨Ø§Ø²Ø³Ø§Ø²ÛŒ Ù„ÛŒØ³Øª Ù†Ù‡Ø§ÛŒÛŒ Ø¨Ø± Ø§Ø³Ø§Ø³ ØªØ±ØªÛŒØ¨ Gemini
        for i_str in selected_indices_str:
            index = int(i_str) - 1 # ØªØ¨Ø¯ÛŒÙ„ "1" Ø¨Ù‡ Ø§ÛŒÙ†Ø¯Ú©Ø³ 0
            # Ù…Ø·Ù…Ø¦Ù† Ù…ÛŒâ€ŒØ´ÙˆÛŒÙ… Ú©Ù‡ Ø§ÛŒÙ†Ø¯Ú©Ø³ Ø¯Ø± Ù…Ø­Ø¯ÙˆØ¯Ù‡ Ù…Ø¬Ø§Ø² Ø§Ø³Øª Ùˆ ØªÚ©Ø±Ø§Ø±ÛŒ Ù†ÛŒØ³Øª
            if 0 <= index < len(documents) and documents[index] not in selected_indices:
                 selected_indices.append(documents[index])

        # 5. Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ø§Ø² Ø§ÛŒÙ†Ú©Ù‡ Ù„ÛŒØ³Øª Ù†Ù‡Ø§ÛŒÛŒ top_k Ø§Ø³Øª
        final_ranked_chunks = selected_indices[:top_k]
        
        st.info(f"âœ… Custom Reranking with Gemini completed. Selected chunks count: {len(final_ranked_chunks)}", icon="ğŸ§ ")
        return final_ranked_chunks
        
    except Exception as e:
        st.error(f"âŒ Custom Gemini Rerank Error: {e}. Falling back to Similarity.")
        # Ø§Ú¯Ø± Ù‡Ø± Ø®Ø·Ø§ÛŒÛŒ Ø±Ø® Ø¯Ø§Ø¯ØŒ Ø¨Ø± Ø§Ø³Ø§Ø³ Similarity Ø§ÙˆÙ„ÛŒÙ‡ Ø¨Ø±Ù…ÛŒâ€ŒÚ¯Ø±Ø¯Ø§Ù†ÛŒÙ…
        documents.sort(key=lambda x: x.get('similarity', 0.0), reverse=True)
        return documents[:top_k]


# ---------------- RAG LOGIC ----------------

def retrieve_rag_context(query: str) -> str:
    """Ø¨Ø§Ø²ÛŒØ§Ø¨ÛŒ Ø²Ù…ÛŒÙ†Ù‡ RAG Ø±Ø§ Ø§Ù†Ø¬Ø§Ù… Ù…ÛŒâ€ŒØ¯Ù‡Ø¯ Ùˆ Custom Reranking Ø±Ø§ Ø§Ø¹Ù…Ø§Ù„ Ù…ÛŒâ€ŒÚ©Ù†Ø¯."""
    
    match_count = TOP_K_RETRIEVAL 
    
    if cloudflare_embed_service is None or supabase is None:
        return "RAG disabled: Cloudflare Embed Client or Supabase initialization failed."
        
    try:
        # 1. ØªÙˆÙ„ÛŒØ¯ Embedding Ú©ÙˆØ¦Ø±ÛŒ (1024 Ø¨ÙØ¹Ø¯)
        qvec = cloudflare_embed_service.embed(query)
        
        current_dim = cloudflare_embed_service.dim 
        if not qvec or len(qvec) != current_dim or all(e == 0.0 for e in qvec):
            return f"RAG disabled: Could not generate a valid {current_dim}-dimension embedding vector using Cloudflare BGE-M3."

        # 2. Ø¬Ø³ØªØ¬ÙˆÛŒ Ø§ÙˆÙ„ÛŒÙ‡ Ø¯Ø± Supabase Ø¨Ø±Ø§ÛŒ Ø¨Ø§Ø²ÛŒØ§Ø¨ÛŒ TOP_K_RETRIEVAL Ú†Ø§Ù†Ú©
        res = supabase.rpc(
            "match_site_pages",
            {"query_embedding": qvec, "match_count": match_count}
        ).execute()

        if not res.data:
            return "No relevant documentation found."
        
        initial_chunks = res.data
        
        # 3. ğŸš¨ Ø§Ø¹Ù…Ø§Ù„ CUSTOM RERANKING Ø¨Ø± Ø±ÙˆÛŒ Ú†Ø§Ù†Ú©â€ŒÙ‡Ø§ÛŒ Ø¨Ø§Ø²ÛŒØ§Ø¨ÛŒ Ø´Ø¯Ù‡
        final_ranked_chunks = custom_rerank_with_gemini(query, initial_chunks, TOP_K_RERANK) 
        
        # 4. Ø³Ø§Ø®Øª Context Ù†Ù‡Ø§ÛŒÛŒ
        chunks = []
        for i, row in enumerate(final_ranked_chunks):
            similarity = row.get("similarity", "N/A")
            
            # ğŸš¨ Ù†Ù…Ø§ÛŒØ´ Ø§ÛŒÙ†Ú©Ù‡ Ø§ÛŒÙ† Ø±ØªØ¨Ù‡ ØªÙˆØ³Ø· Gemini Ø¯Ø§Ø¯Ù‡ Ø´Ø¯Ù‡ Ø§Ø³Øª
            rerank_info = f"Rank: {i+1} (Gemini)" 
            
            source_info = row.get("url", "local").split('//')[-1].split('/')[0]
            title = row.get("title", "Untitled")
            content = row.get("content", "")
            
            chunks.append(f"--- Chunk {i+1} (Title: {title}, Source: {source_info}, Info: {rerank_info}, Sim: {similarity:.3f}) ---\n{content}")
            
        return "\n\n".join(chunks)
        
    except Exception as e:
        return f"RAG error: {e}"


def generate_rag_response(user_query: str, history: List[Dict[str, str]], context: str) -> str:
    # ... (Ù…Ù†Ø·Ù‚ ØªÙˆÙ„ÛŒØ¯ Ù¾Ø§Ø³Ø® Ø¨Ø§ Gemini Ø¨Ø¯ÙˆÙ† ØªØºÛŒÛŒØ±) ...
    if gemini_service is None:
        return "LLM service is not initialized."
        
    is_rag_active_and_valid = not context.startswith("RAG disabled:") and not context.startswith("RAG error:")
    is_context_useful = is_rag_active_and_valid and len(context.strip()) > 50

    base_sys_prompt = (
        "Ø´Ù…Ø§ ÛŒÚ© Ø¯Ø³ØªÛŒØ§Ø± RAG Ù…ØªØ®ØµØµ Ùˆ Ù…Ø³Ù„Ø· Ø¨Ù‡ Ø²Ø¨Ø§Ù† ÙØ§Ø±Ø³ÛŒ Ù‡Ø³ØªÛŒØ¯. "
        "Ù‡Ø¯Ù Ø§ØµÙ„ÛŒ Ø´Ù…Ø§ ØªÙˆÙ„ÛŒØ¯ Ù¾Ø§Ø³Ø®â€ŒÙ‡Ø§ÛŒ Ú©Ø§Ù…Ù„ØŒ Ø¯Ù‚ÛŒÙ‚ Ùˆ Ø³Ø§Ø®ØªØ§Ø±ÛŒØ§ÙØªÙ‡ Ø¨Ø± Ø§Ø³Ø§Ø³ Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø§Ø±Ø§Ø¦Ù‡ Ø´Ø¯Ù‡ Ø§Ø³Øª."
        "\n\n--- Ù‚ÙˆØ§Ù†ÛŒÙ† ØªÙˆÙ„ÛŒØ¯ Ù¾Ø§Ø³Ø® ---\n"
        "1. **Ù„Ø­Ù†:** Ù‡Ù…ÛŒØ´Ù‡ Ù¾Ø§Ø³Ø®ÛŒ Ø¯ÙˆØ³ØªØ§Ù†Ù‡ØŒ Ø¢Ú¯Ø§Ù‡Ø§Ù†Ù‡ Ùˆ Ø¯Ø± Ø¹ÛŒÙ† Ø­Ø§Ù„ Ø­Ø±ÙÙ‡â€ŒØ§ÛŒ Ø§Ø±Ø§Ø¦Ù‡ Ø¯Ù‡ÛŒØ¯."
        "2. **Ø³Ø§Ø®ØªØ§Ø±Ø¯Ù‡ÛŒ:** Ù¾Ø§Ø³Ø®â€ŒÙ‡Ø§ Ø±Ø§ Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² **Ø¨ÙˆÙ„ÙØª Ù¾ÙˆÛŒÙ†Øªâ€ŒÙ‡Ø§** ÛŒØ§ **Ø³Ø±ÙØµÙ„â€ŒÙ‡Ø§ÛŒ Markdown (##)** Ø³Ø§Ø®ØªØ§Ø±Ø¯Ù‡ÛŒ Ú©Ù†ÛŒØ¯ ØªØ§ Ø®ÙˆØ§Ù†Ø§ÛŒÛŒ Ø¨Ù‡ Ø­Ø¯Ø§Ú©Ø«Ø± Ø¨Ø±Ø³Ø¯. "
        "3. **Ø¹Ø¯Ù… ØªÚ©Ø±Ø§Ø±:** Ø§Ø² ØªÚ©Ø±Ø§Ø± Ù…Ø³ØªÙ‚ÛŒÙ… Ø¬Ù…Ù„Ø§Øª Ù…ÙˆØ¬ÙˆØ¯ Ø¯Ø± Context Ø®ÙˆØ¯Ø¯Ø§Ø±ÛŒ Ú©Ù†ÛŒØ¯Ø› Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø±Ø§ ØªØ±Ú©ÛŒØ¨ Ú©Ø±Ø¯Ù‡ Ùˆ Ø¨Ø§Ø²Ù†ÙˆÛŒØ³ÛŒ Ú©Ù†ÛŒØ¯."
        "4. **ØªØ§Ø±ÛŒØ®Ú†Ù‡:** Ù‡Ù…ÛŒØ´Ù‡ Ø¨Ù‡ ØªØ§Ø±ÛŒØ®Ú†Ù‡â€ŒÛŒ Ù…Ú©Ø§Ù„Ù…Ù‡ ØªÙˆØ¬Ù‡ Ú©Ù†ÛŒØ¯ ØªØ§ Ø¨ØªÙˆØ§Ù†ÛŒØ¯ Ø¨Ù‡ Ø³Ø¤Ø§Ù„Ø§Øª Ø¯Ù†Ø¨Ø§Ù„Ù‡â€ŒØ¯Ø§Ø± (Follow-up) Ù¾Ø§Ø³Ø® Ø¯Ù‡ÛŒØ¯."
        "5. **Ø§ÙˆÙ„ÙˆÛŒØª:** Ø§Ø¨ØªØ¯Ø§ Ø¨Ù‡ Ø³Ø¤Ø§Ù„ Ø§ØµÙ„ÛŒ Ú©Ø§Ø±Ø¨Ø± Ù¾Ø§Ø³Ø® Ø¯Ù‡ÛŒØ¯ØŒ Ø³Ù¾Ø³ Ø¬Ø²Ø¦ÛŒØ§Øª Ù…Ø±ØªØ¨Ø· Ø±Ø§ Ø§Ø¶Ø§ÙÙ‡ Ú©Ù†ÛŒØ¯."
    )

    if is_context_useful:
        sys_prompt = (
            f"{base_sys_prompt} Ù‡Ø¯Ù Ø§ØµÙ„ÛŒ Ø´Ù…Ø§ Ù¾Ø§Ø³Ø® Ø¨Ù‡ Ø³Ø¤Ø§Ù„ Ú©Ø§Ø±Ø¨Ø± Ø¨Ø± Ø§Ø³Ø§Ø³ **ØªÙ†Ù‡Ø§** 'RAG CONTEXT' Ø§Ø±Ø§Ø¦Ù‡ Ø´Ø¯Ù‡ Ø§Ø³Øª. "
            "Ø´Ù…Ø§ **Ø¨Ø§ÛŒØ¯** Ù¾Ø§Ø³Ø® Ø®ÙˆØ¯ Ø±Ø§ Ø¨Ø§ Ø¹Ø¨Ø§Ø±Øª: '**Ø¨Ø± Ø§Ø³Ø§Ø³ Ù…Ù†Ø¨Ø¹ Ø¯Ø§Ù†Ø´ØŒ**' Ø´Ø±ÙˆØ¹ Ú©Ù†ÛŒØ¯ Ø§Ú¯Ø± Ù¾Ø§Ø³Ø® Ø±Ø§ Ø¯Ø± Context Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯ÛŒØ¯. "
            "Ø§Ú¯Ø± Context Ù¾Ø§Ø³Ø® Ø±Ø§ Ù†Ø¯Ø§Ø±Ø¯ØŒ Ø¨Ø§ÛŒØ¯ Ø¨Ù‡ ÙˆØ¶ÙˆØ­ Ø¨ÛŒØ§Ù† Ú©Ù†ÛŒØ¯: 'Ù…ØªØ£Ø³ÙØ§Ù†Ù‡ØŒ Ù…Ù†Ø¨Ø¹ Ø¯Ø§Ù†Ø´ Ù…Ù† Ø´Ø§Ù…Ù„ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù„Ø§Ø²Ù… Ø¨Ø±Ø§ÛŒ Ù¾Ø§Ø³Ø® Ø¨Ù‡ Ø§ÛŒÙ† Ø³Ø¤Ø§Ù„ Ù†ÛŒØ³Øª.' "
            "Ø§Ø² Ø¯Ø§Ù†Ø´ Ø¹Ù…ÙˆÙ…ÛŒ Ø®ÙˆØ¯ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù†Ú©Ù†ÛŒØ¯ Ù…Ú¯Ø± Ø§ÛŒÙ†Ú©Ù‡ Context Ø¨Ù‡ ÙˆØ¶ÙˆØ­ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù†Ø§Ù…Ø±ØªØ¨ØªÛŒ Ø§Ø±Ø§Ø¦Ù‡ Ø¯Ù‡Ø¯ ÛŒØ§ Ø®Ø§Ù„ÛŒ Ø¨Ø§Ø´Ø¯."
            "\n\n--- RAG CONTEXT ---\n"
            f"{context}\n---"
        )
    else:
        sys_prompt = (
            f"{base_sys_prompt} 'RAG CONTEXT' Ú©Ø§ÙÛŒ ÛŒØ§ Ù…Ø±ØªØ¨Ø·ÛŒ ÛŒØ§ÙØª Ù†Ø´Ø¯. "
            "Ø´Ù…Ø§ Ø¨Ø§ÛŒØ¯ Ø³Ø¤Ø§Ù„ Ú©Ø§Ø±Ø¨Ø± Ø±Ø§ Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² **Ø¯Ø§Ù†Ø´ Ø¹Ù…ÙˆÙ…ÛŒ** Ùˆ **ØªØ§Ø±ÛŒØ®Ú†Ù‡ Ù…Ú©Ø§Ù„Ù…Ù‡** Ø®ÙˆØ¯ Ù¾Ø§Ø³Ø® Ø¯Ù‡ÛŒØ¯. "
            "Ú†ÙˆÙ† RAG ÙØ¹Ø§Ù„ Ù†ÛŒØ³ØªØŒ **Ù‡ÛŒÚ† Ø§Ø´Ø§Ø±Ù‡â€ŒØ§ÛŒ** Ø¨Ù‡ Ù…Ù†Ø¨Ø¹ Ø¯Ø§Ù†Ø´ (Ù…Ø§Ù†Ù†Ø¯ 'Ø¨Ø± Ø§Ø³Ø§Ø³ Ù…Ù†Ø¨Ø¹ Ø¯Ø§Ù†Ø´') ÛŒØ§ Context Ø¯Ø± Ù¾Ø§Ø³Ø® Ø®ÙˆØ¯ Ù†Ú©Ù†ÛŒØ¯. "
            "ÙÙ‚Ø· Ù…Ø³ØªÙ‚ÛŒÙ…Ø§Ù‹ Ø¨Ù‡ Ø³Ø¤Ø§Ù„ Ú©Ø§Ø±Ø¨Ø± Ù¾Ø§Ø³Ø® Ø¯Ù‡ÛŒØ¯."
        )

    response_text = gemini_service.generate(sys_prompt, history, user_query)
    
    return response_text


# ---------------- STREAMLIT APP ----------------
def main_streamlit_app():
    
    st.title(" ğŸ“š Custom Gemini Rerank Chatbot ")
    
    if cloudflare_embed_service and gemini_service:
        st.info(f"ğŸ’¡ **ÙˆØ¶Ø¹ÛŒØª:** Ø­Ø§ÙØ¸Ù‡ Ù…Ú©Ø§Ù„Ù…Ù‡â€ŒØ§ÛŒ ÙØ¹Ø§Ù„ØŒ **Embedding:** {EMBED_MODEL} ({cloudflare_embed_service.dim} Dim), **Reranking:** {RERANK_MODEL} (Retrieval: {TOP_K_RETRIEVAL} -> Final: **{TOP_K_RERANK}**).", icon="ğŸ§ ")
    else:
         st.warning("âš ï¸ RAG (Embedding/LLM) ØºÛŒØ±ÙØ¹Ø§Ù„ Ø§Ø³Øª (Ø®Ø·Ø§ÛŒ API/ØªÙ†Ø¸ÛŒÙ…Ø§Øª).")
         
    if "messages" not in st.session_state:
        st.session_state["messages"] = [
            {"role": "assistant", "content": "Ø³Ù„Ø§Ù…ØŒ Ù…Ù† ÛŒÚ© Ø¯Ø³ØªÛŒØ§Ø± RAG Ù‡Ø³ØªÙ…. Ù„Ø·ÙØ§Ù‹ Ø³Ø¤Ø§Ù„ Ø®ÙˆØ¯ Ø±Ø§ Ø¨Ù¾Ø±Ø³ÛŒØ¯.", "timestamp": datetime.now().isoformat()} 
        ]
    
    for msg in st.session_state.messages:
        with st.chat_message(msg["role"]):
            st.markdown(msg["content"])
            
            if "timestamp" in msg:
                try:
                    dt_obj = datetime.fromisoformat(msg["timestamp"])
                    time_str = dt_obj.strftime("%Y/%m/%d - %H:%M:%S")
                    alignment = "right" if msg["role"] == "user" else "left"
                    st.markdown(f'<p style="color: grey; font-size: small; text-align: {alignment}; margin-bottom: 0px;">{time_str}</p>', unsafe_allow_html=True)
                except ValueError:
                    st.markdown('<p style="color: red; font-size: small;">Ø²Ù…Ø§Ù† Ù†Ø§Ù…Ø¹ØªØ¨Ø±</p>', unsafe_allow_html=True)


    if prompt := st.chat_input("Ø³Ø¤Ø§Ù„ Ø®ÙˆØ¯ Ø±Ø§ Ø§ÛŒÙ†Ø¬Ø§ Ø¨Ù†ÙˆÛŒØ³ÛŒØ¯..."):
        current_time = datetime.now().isoformat()
        
        st.session_state.messages.append({"role": "user", "content": prompt, "timestamp": current_time})
        with st.chat_message("user"):
            st.markdown(prompt)
            dt_obj = datetime.fromisoformat(current_time)
            time_str = dt_obj.strftime("%Y/%m/%d - %H:%M:%S")
            st.markdown(f'<p style="color: grey; font-size: small; text-align: right; margin-bottom: 0px;">{time_str}</p>', unsafe_allow_html=True)


        with st.chat_message("assistant"):
            response_placeholder = st.empty() 
            
            with st.spinner("â³ Ø¯Ø± Ø­Ø§Ù„ Ø¨Ø§Ø²ÛŒØ§Ø¨ÛŒ Ø¯Ø§Ù†Ø´ Ùˆ ØªÙˆÙ„ÛŒØ¯ Ù¾Ø§Ø³Ø®..."):
                
                # 1. Ø¨Ø§Ø²ÛŒØ§Ø¨ÛŒ Context (Ø´Ø§Ù…Ù„ 5 Ú†Ø§Ù†Ú© Ù…Ø±ØªØ¨ Ø´Ø¯Ù‡)
                rag_context = retrieve_rag_context(prompt) 
                
                # 2. ØªÙˆÙ„ÛŒØ¯ Ù¾Ø§Ø³Ø® Ø¨Ø§ Context Ú©Ø§Ù…Ù„
                response = generate_rag_response(prompt, st.session_state.messages, rag_context)

                response_placeholder.markdown(response)
                
                assistant_time = datetime.now().isoformat()
                
                st.session_state.messages.append({"role": "assistant", "content": response, "timestamp": assistant_time})
                
                dt_obj = datetime.fromisoformat(assistant_time)
                time_str = dt_obj.strftime("%Y/%m/%d - %H:%M:%S")
                st.markdown(f'<p style="color: grey; font-size: small; text-align: left; margin-bottom: 0px;">{time_str}</p>', unsafe_allow_html=True)


                with st.expander("ğŸ“ Ø¬Ø²Ø¦ÛŒØ§Øª Ù¾Ø±Ø¯Ø§Ø²Ø´ Ùˆ Ù…Ù†Ø¨Ø¹ Ø¯Ø§Ù†Ø´"):
                    
                    is_rag_active_and_valid = not rag_context.startswith("RAG disabled:") and not rag_context.startswith("RAG error:")
                    
                    st.markdown(f"**Ù…Ø¯Ù„ LLM:** `{GEMINI_MODEL}`")
                    st.markdown(f"**Ù…Ø¯Ù„ Embedding:** `{EMBED_MODEL}` (Dim: {cloudflare_embed_service.dim if cloudflare_embed_service else 'N/A'})")
                    st.markdown(f"**Ù…Ø¯Ù„ Reranker:** `{RERANK_MODEL}`")
                    st.markdown(f"**Ø¨Ø§Ø²ÛŒØ§Ø¨ÛŒ Ø§ÙˆÙ„ÛŒÙ‡ (Similarity):** {TOP_K_RETRIEVAL} Ú†Ø§Ù†Ú© | **Ø¨Ø§Ø²ÛŒØ§Ø¨ÛŒ Ù†Ù‡Ø§ÛŒÛŒ (Rerank):** **{TOP_K_RERANK}** Ú†Ø§Ù†Ú©")
                    st.markdown("---")
                    
                    if "**Ø¨Ø± Ø§Ø³Ø§Ø³ Ù…Ù†Ø¨Ø¹ Ø¯Ø§Ù†Ø´ØŒ**" in response and is_rag_active_and_valid:
                        st.info("âœ… **ÙˆØ¶Ø¹ÛŒØª Agent:** Agent Ø§Ø² Ù…Ù†Ø¨Ø¹ Ø¯Ø§Ù†Ø´ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ø±Ø¯Ù‡ Ùˆ Ù¾Ø§Ø³Ø® Ø±Ø§ ØªÙˆÙ„ÛŒØ¯ Ú©Ø±Ø¯Ù‡ Ø§Ø³Øª.", icon="ğŸ“š")
                    elif is_rag_active_and_valid and "No relevant documentation found." in rag_context:
                        st.warning("âš ï¸ **ÙˆØ¶Ø¹ÛŒØª Agent:** Context Ø¨Ø§Ø²ÛŒØ§Ø¨ÛŒ Ø´Ø¯ØŒ Ø§Ù…Ø§ Ù‡ÛŒÚ† Ø³Ù†Ø¯ Ù…Ø±ØªØ¨Ø·ÛŒ Ø¯Ø± Ø¯ÛŒØªØ§Ø¨ÛŒØ³ Ù¾ÛŒØ¯Ø§ Ù†Ø´Ø¯. Agent Ø§Ø² Ø¯Ø§Ù†Ø´ Ø¹Ù…ÙˆÙ…ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ø±Ø¯.")
                    elif is_rag_active_and_valid:
                        st.warning("âš ï¸ **ÙˆØ¶Ø¹ÛŒØª Agent:** Ù…Ù†Ø¨Ø¹ Ø¯Ø§Ù†Ø´ Ø¨Ø§Ø²ÛŒØ§Ø¨ÛŒ Ø´Ø¯ØŒ Ø§Ù…Ø§ Agent ØªØµÙ…ÛŒÙ… Ú¯Ø±ÙØª Ø§Ø² Ø¢Ù† Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù†Ú©Ù†Ø¯. (ÛŒØ§ Context Ú©Ø§ÙÛŒ Ù†Ø¨ÙˆØ¯)")
                    else:
                        error_message = rag_context.split(': ', 1)[-1]
                        st.error(f"âŒ **ÙˆØ¶Ø¹ÛŒØª Agent:** Ø¨Ø§Ø²ÛŒØ§Ø¨ÛŒ Ù…Ù†Ø¨Ø¹ Ø¯Ø§Ù†Ø´ Ù†Ø§Ù…ÙˆÙÙ‚ Ø¨ÙˆØ¯. \n\n**Ø¯Ù„ÛŒÙ„:** {error_message}", icon="ğŸš¨")
                    
                    st.markdown("---")
                    st.markdown(f"**ØªØ¹Ø¯Ø§Ø¯ Ù¾ÛŒØ§Ù…â€ŒÙ‡Ø§ÛŒ Ø¯Ø± Ø­Ø§ÙØ¸Ù‡ (Memory):** `{len(st.session_state.messages)}`")
                    
                    if rag_context and is_rag_active_and_valid:
                        # Ù†Ù…Ø§ÛŒØ´ Ú†Ø§Ù†Ú© Ø¨Ø±ØªØ±
                        start_index = rag_context.find("--- Chunk 1 ")
                        end_index_2 = rag_context.find("--- Chunk 2 ", start_index)
                        
                        if start_index != -1: 
                            if end_index_2 == -1:
                                display_chunk = rag_context[start_index:].strip()
                            else:
                                display_chunk = rag_context[start_index:end_index_2].strip()

                            st.text_area("ğŸ“„ **Ù…Ø±ØªØ¨Ø·â€ŒØªØ±ÛŒÙ† Ù…Ù†Ø¨Ø¹ (Context):**", display_chunk, height=300)
                            st.text_area("âœ¨ **Context Ú©Ø§Ù…Ù„ (5 Ú†Ø§Ù†Ú©) Ø§Ø±Ø³Ø§Ù„ÛŒ Ø¨Ù‡ Gemini**", rag_context, height=100, help="Ø§ÛŒÙ† Ù…ØªÙ†ÛŒ Ø§Ø³Øª Ú©Ù‡ Gemini Ø¨Ø±Ø§ÛŒ ØªÙˆÙ„ÛŒØ¯ Ù¾Ø§Ø³Ø® Ø´Ù…Ø§ Ø§Ø² Ø¢Ù† Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ø±Ø¯Ù‡ Ø§Ø³Øª.")
                        else:
                            st.text_area("âœ¨ **Context Ú©Ø§Ù…Ù„ (5 Ú†Ø§Ù†Ú©) Ø§Ø±Ø³Ø§Ù„ÛŒ Ø¨Ù‡ Gemini**", rag_context, height=100)
                    else:
                        st.markdown("**Context Ù…Ø±ØªØ¨Ø·ÛŒ Ø¨Ø±Ø§ÛŒ Ù†Ù…Ø§ÛŒØ´ ÛŒØ§ÙØª Ù†Ø´Ø¯.**")

if __name__ == "__main__":
    if 'cloudflare_embed_service' not in globals() or cloudflare_embed_service is None or gemini_service is None:
        cloudflare_embed_service, gemini_service = get_clients() 
    
    main_streamlit_app()
